{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Solucion_Ejercicio_Wikipedia.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3vKPTOoGpy6"
      },
      "source": [
        "##Solución\n",
        "\n",
        "Se realiza la iteración sobre el dataset y se va extrayendo de la wikipedia la descripción en base a la columna full_name.\n",
        "Primeramente se realiza una comprobación de si en la respuesta de la wikipedia existe o no una descripción de ese personaje. Si no existe, no se almacena en el nuevo dataset.\n",
        "\n",
        "Esta celda de código tardará en ejecutarse debido a la cantidad de peticiones a la api de Wikipedia. Puede ir viéndose el progreso en el archivo \"logExtractBiographies.txt\" que se crea el propio script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtjJlwH8Gapp"
      },
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import sys\n",
        "\n",
        "originalData = pd.read_csv(\"pantheon1database.csv\")\n",
        "\n",
        "#Este objeto logger se emplea únicamente para ir guardando en un fichero un registro de las tuplas que vamos procesando. (Por si sucede algún error)\n",
        "logger= open(\"logExtractBiographies.txt\",\"w\")\n",
        "logger.write(\"Log for Extract Bios\\n\")\n",
        "logger.close();\n",
        "\n",
        "#Dataset en el que se van a ir añadiendo las descripciones asociadas a los nombres\n",
        "finalDataset = pd.DataFrame(columns = ['full_name', 'wiki_description'])\n",
        "\n",
        "actualTuple = 1\n",
        "numTuples = originalData.shape[0]\n",
        "\n",
        "for row in originalData.itertuples():\n",
        "    S = requests.Session()\n",
        "\n",
        "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
        "    \n",
        "    #row[1] contiene el id del articulo en wikipedia. row[2] el nombre del personaje histórico\n",
        "    PARAMS = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"titles\": row[2],\n",
        "        \"prop\": \"extracts\",\n",
        "        \"explaintext\": \"1\",\n",
        "        \"exintro\": \"1\"\n",
        "    }\n",
        "    try:\n",
        "        \n",
        "        R = S.get(url=URL, params=PARAMS)\n",
        "\n",
        "        DATA = R.json()\n",
        "        if \"extract\" in DATA['query']['pages'][list(DATA['query']['pages'].keys())[0]]:\n",
        "            wiki_description = DATA['query']['pages'][list(DATA['query']['pages'].keys())[0]]['extract']\n",
        "            if (wiki_description ==\"\"):\n",
        "                wiki_description=None\n",
        "        else:\n",
        "            wiki_description =None\n",
        "        #añadimos la tupla con el nombre y la descripción al dataset si es que existe\n",
        "        if (wiki_description != None):\n",
        "          finalDataset = finalDataset.append({'full_name':row[2], 'wiki_description':wiki_description}, ignore_index=True)\n",
        "\n",
        "        logger= open(\"logExtractBiographies.txt\",\"a\")\n",
        "        logger.write(\"Retrieved wiki about \"+ str(row[2])+ \" \"+str(actualTuple)+ \" / \"+str(numTuples)+\"\\n\")\n",
        "        logger.close()\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger= open(\"logExtractBiographies.txt\",\"a\")\n",
        "        logger.write(\"Oops!\"+ str(e.__class__)+ \" occurred.\"+\"\\n\")\n",
        "        logger.close()\n",
        "        \n",
        "    actualTuple = actualTuple+1\n",
        "\n",
        "#guardamos en un csv el nuevo dataset\n",
        "finalDataset.to_csv(\"pantheonWikipediaData.csv\",  index=False)\n",
        "\n",
        "    "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2lpCzPtGhV-"
      },
      "source": [
        "\n",
        "Si queremos hacer el join de los dos dataset (el original y el nuevo que contiene las descripciones):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2q8k0vUrGaps"
      },
      "source": [
        "newData = pd.merge(originalData, finalDataset, on=\"full_name\")\n",
        "newData.to_csv(\"pantheonDatabaseConBiografias.csv\",  index=False)"
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}